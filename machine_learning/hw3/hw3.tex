\documentclass[12pt]{article}% uses letterpaper by default

%---------- Uncomment one of them ------------------------------
\usepackage[includeheadfoot, top=.5in, bottom=.5in, hmargin=1in]{geometry}

% \usepackage[a5paper, landscape, twocolumn, twoside,
%    left=2cm, hmarginratio=2:1, includemp, marginparwidth=43pt,
%    bottom=1cm, foot=.7cm, includefoot, textheight=11cm, heightrounded,
%    columnsep=1cm, dvips,  verbose]{geometry}
%---------------------------------------------------------------
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage{url}
\pagestyle{fancy}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{}
%\doublespacing
\singlespacing

\rhead{Andrew Emerick\\ UNI: aje2123 \\ HW 03 \\ STAT W4400}
\renewcommand{\rightmark}{}


\begin{document}

\section{Problem 1}
\subsection{Description of Code and Methods}
The AdaBoost algorithm and supporting functions are implemented in the included code, along with a second R file to load data, run AdaBoost, and plot the results. The AdaBoost algorithm itself is implemented in the \texttt{AdaBoost} function, which takes in the data, class labels, and desired number of decision stumps (DS) as arguments, returning voting weights, DS parameters, and error on training data as a function of B. These can then be fed to the aggregated classifier (\texttt{agg\_class}) to classify new data. 

\texttt{AdaBoost} uses the functions \texttt{train} to train the decision stumps and \texttt{classify} to use the DS parameters to classify the data. The \texttt{calc\_error} function calculates the misclassification rate given two sets of class labels. \texttt{calc\_error} accepts an optional array of weights to calculate weighted error as defined in part b and equation 2 in the homework. \texttt{train} performs a grid search for the best DS parameter triplet of (j, $\theta$, m). A range of $\theta$ values are checked for each axis. For n data points, the n-1 $\theta$ values are chosen at halfway in between every data point, with 2 additional $\theta$ values below/above the minimum/maximum data point value. This is done for every axis. Weighted error is computed for every possible parameter triplet (with m as either +1 or -1), and the best parameters are returned (as defined in equation 2). For a d dimensional data set of n data points, this corresponds to a total of (n+1)$\times$d$\times$2 parameter triplets to test; for n = 200 and d = 256, this is 102,400 parameter triplets. In effect, for (as an example) B = 10 and K = 5 cross validation folds, this corresponds to over 5 million parameter triplets. Run time is reduced by taking advantage of R programming syntax and vector/matrix operations, minimizing the number of for loops needed.

As asked, a K-fold cross validation method is implemented to compute errors on training and test data as a function of B in order to find the ideal B value. With cross validation, the algorithm takes a while to evaluate (about 1 minute for B=5), as \texttt{AdaBoost} must be called K times. \texttt{CV\_AdaBoost} is a wrapper around \texttt{AdaBoost} to perform the cross validation. This takes in the entire data set and performs an initial random sorting of the data that persists throughout the CV process. The data is split into K folds (I used K=5), with 1 test fold and K-1 training folds. An AdaBoost classifier is trained on the selected training data, and the associated test error is calculated on the test data as a function of B. This is done K times until every fold is a test fold once. The final training and test errors are taken as the cross validation training and test errors averaged over the K folds.

\subsection{Results of CV with AdaBoost}
Fig.~\ref{fig:errors} shows the misclassification rate of the AdaBoost algorithm as averaged over K=5 cross validation folds. The training error rate is given in black, with the test rate in blue. The misclassification rate of the training data becomes zero fairly quickly, at B $\ge$ 17. At this point, \texttt{AdaBoost} becomes inefficient at improving the quality of the resulting classifier, and thus the errors on the test data are noisy as a function of B. The test errors at this point are governed by the nuances of the data used in the training vs. test folds. However, the amplitude of fluctuations about the mean do decrease towards larger B, but this is only a small improvement. By about B = 50, there is no more readily discernible improvement. The final test error fluctuates about a \textbf{mean final error rate of 0.053}, as shown. And again, in this case \textbf{B = 17 is the minimum number of DS necessary}; larger B (around 25-50) would be my recommended number of decision stumps to train.



\begin{figure}[tb]
\center
\includegraphics[width=0.75\linewidth]{final_error.pdf}
\caption{Misclassification rates of aggregated AdaBoost classifier as a function of the 
number of decision stumps used, as averaged over a K=5 fold cross validation scheme. As
shown, training error becomes zero at B=17. At this point, classifier can only improve
with more / better training data... hence the noise in the test errors around a mean
of about 0.05}
\label{fig:errors}
\end{figure}

\section{Problem 2}

The q = 0.5 $l_q$-regression (as shown on the left) encourages sparse elements as compared to the q = 4 regression. This is illustrated fairly well by the diagram. $\hat{\beta}$ designates the $\beta$ parameter values obtained by the least squared error (LSE) fit (the $l_q$ norm regression methods are modifications of the LSE), and the contours around $\hat{\beta}$ give fixed LSE cost. The contours centered on the origin show fixed cost for the $l_q$ norm, and thus the constraints the $l_q$ norm regression places on $\beta$. The resulting $\beta$ parameters obtained by the regression are those that minimize the total cost. In this diagram, that means the first point where the LSE contours touch the $l_q$ norm contours. In other words, for a fixed LSE contour, that is the point that lies closest to the origin on (or interior to) a given contour from the $l_q$ norm. 

\textbf{a:} For smaller q, the contours become sharp / pointed at the axis, with steeper and steeper falloffs on either side. This means that, for larger q, there is more contour sitting closer to the LSE contours in the region where $\beta_1$ and $\beta_2$ are non-zero. Given what I said above, this means that the minimum total error is more likely to occur in the region with non-zero $\beta_1$ and $\beta_2$. This is even clearer in visualizing the limiting cases of very small q (a plus sign) and very large q (a square). Again, \textbf{the q = 0.5 $l_q$-regression encourages sparse elements when compared to the q = 4 $l_q$-regression}.

\textbf{b:} Again, given what is said above, the points that would minimize cost are: \textbf{$x_3$ for q=0.5} and \textbf{$x_4$ for q=4}. Since all 5 points lie on the same LSE contour, the LSE error component is fixed. Therefore (echoing the above argument), for a fixed LSE error, the point that minimizes the total cost is that which is closest to the origin on or interior to the smallest $l_q$ contour that touches at least one of the $x$ points. In both diagrams, $x_3$ and $x_4$ are closest to the origin, but $x_4$ is (slightly) closer. However, for q = 0.5, $x_3$ lies on a smaller $l_q$ contour than all of the other points, and thus minimizes the total cost. Although $x_3$ and $x_5$ lie on the drawn contour in the q = 4 diagram, since $x_4$ is closer to the origin, it lies on a smaller $l_q$ contour (i.e. interior to the one drawn), and thus minimizes the total error for q = 4.



\end{document}
